\documentclass{article}
\usepackage{fullpage}
\usepackage{wrapfig}
\usepackage[colorlinks,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsmath,amsfonts}
\usepackage{graphicx,color}
\usepackage{listings}
\lstset{language=Python,basicstyle=\ttfamily\footnotesize}
\usepackage{verbatim}

\newcommand{\dealii}{{\textsc{deal.II}}}
\newcommand{\pfrst}{{\normalfont\textsc{p4est}}}
\newcommand{\trilinos}{{\textsc{Trilinos}}}
\newcommand{\aspect}{\textsc{Aspect}}
\newcommand{\petsc}{\textsc{PETSc}}

\newtheorem{remark}{Remark}

\setlength{\emergencystretch}{20pt}

\lstset{language=C++,%
  basicstyle=\small\ttfamily,%
  escapeinside={\%*}{*)},%
  keywordstyle={},%
  captionpos=b,%
  frame=single,basicstyle=\footnotesize%
}
\lstset{language=Python,%
  basicstyle=\small\ttfamily,%
  escapeinside={\%*}{*)},%
  keywordstyle={},%
  captionpos=b,%
  frame=single,basicstyle=\footnotesize%
}
%


\begin{document}

%\runningheads{B. Turcksin, T. Heister, W. Bangerth}{Clone and graft: Testing whole applications}

\title{Clone and graft:\\ Testing whole scientific
  applications as they are built}

\author{Bruno Turcksin\footnote{Department of Mathematics, Texas A\&M
    University, College Station, TX 77843-3368, USA, \url{???}}
\and
   Timo Heister\footnote{Mathematical Sciences, O-110 Martin Hall, Clemson University,
     Clemson, SC 29634-095, USA, \url{heister@clemson.edu}}
\and
   Wolfgang Bangerth\footnote{Corresponding author. Department of Mathematics, Texas A\&M
     University, College Station, TX 77843-3368, USA, \url{bangerth@math.tamu.edu}}}


\maketitle

\section{Introduction}

Our group has been building scientific software for more than a decade and a
half by now. Over the years we have developed many procedures that help us
test our software extensively but our expertise was mostly in the development
of \textit{libraries} for numerical methods -- specifically, the \dealii{}
library for finite element computations (see
\url{http://www.dealii.org/}). Libraries are of course the foundation of
almost every single scientific code, starting with ``simple'' ones such as
BLAS and LAPACK to very much more complex ones such as \petsc{}, \trilinos{}, or
\dealii{}.

Developing testing schemes for libraries is reasonably well understood. So,
when we ventured in the area of building applications for scientific purposes
on top of the libraries we had created, we had initially thought that one can
use the same approaches as one uses for libraries -- but this turned out to
not work.

Libraries are relatively easy to test because they export large numbers of
functions that one can call individually from small test programs. These are
often called \textit{unit tests} and are typically complemented by integration
tests to verify that combinations of classes work well together. Over the
years we have written about 3,000 of these that are run in multiple
configurations with every single commit to \dealii{} -- on multiple machines,
with different compilers and dependencies, and several times a day. Despite
all of the experience we have gained building this machinery around \dealii{},
the same strategy does not work for whole applications because applications do
not usually export many possible entry points. Rather, they usually only
present a single interface -- the command line, a graphical user interface, or
input files -- and then run through a large fraction of the entire code base
in computing output. One cannot typically just call a single function in an
application by presenting a magic input file, in the same way as one would
call a function in a library by presenting a magic \texttt{main()} function in
a unit test that just sets up some data and then calls the function in
question.

Consequently, different approaches are necessary. In the following, we will
lay out what we have learned from building and testing the \aspect{} code for the
geodynamics community over the past few years. In the following, let us first
talk briefly about what \aspect{} does and how the way it is used is
prototypical for scientific codes (Section~\ref{...}). At the same time,
\aspect{} is a very complex code and just installing it is non-trivial;
consequently, it does not serve the purpose of this book well and we will
present a simple model problem and model code in Section~\ref{...} that we
will then use throughout the rest of the chapter. Section~\ref{...} then
discusses how we can write tests alongside developing the code -- using the
\textit{clone and graft} technique of creating test cases. ...COMPLETE...


\section{\aspect{}: the Advanced Solver for Problems in Earth ConvecTion}

\begin{wrapfigure}{R}{0.44\textwidth}
  \begin{center}
    \vspace*{-24pt}
    \includegraphics[width=0.42\textwidth,height=0.42\textwidth]{figures/aspect.png}
    \vspace*{-12pt}
  \end{center}
  \caption{\it Snapshot of the temperature field of an \aspect{} simulation
    showing convection in the Earth mantle.}
  \vspace*{-3mm}
  \label{fig:aspect}
\end{wrapfigure}
The code that made us think about how to test whole applications is called
\aspect{} -- short for the Advanced Solver for Problems in Earth ConvecTion
\cite{...}. It is a program that simulates how material moves around the Earth
mantle (i.e., the region between the metallic core of Earth and the plates at
the surface on which we live). While the material in the mantle is solid rock,
it is hot and under enormous pressure and can deform with velocities of a few
centimeters per year. On time scales of millions of years, it therefore
behaves like a fluid. Since it is heated from below and cooled from above, it
shows the same kind of behavior as a pot on the stove: blobs of hot material
rise up, cool at the surface, and blobs of cool material fall down. In other
words, it convects. The details of trying to simulate this on a computer go
beyond what we want to discuss here, but it is worth showing a picture and
linking to the videos at \url{http://www.youtube.com/embed/j63MkEc0RRw} and
\url{http://www.youtube.com/embed/EJJ6f4hmDPU} simply because they are pretty.

\aspect{} is a large code: At the time of writing this chapter, it has 297
source files with 69,704 lines of code. What's more, it builds on the
\dealii{} library that has some 500,000 lines of code, the \trilinos{} library
with 3,500,000 lines, and a few other but smaller libraries. At the same time,
this size is not uncommon for large scientific codes, and certainly not for
commercial codes.

Like many academic codes, \aspect{} has no graphical user interface but
instead is driven by input files; it is also designed in such a way that it is
easy to add functionality through plugins, i.e., self-contained source files
that simply implement a class derived from some base class and that is then
registered in \aspect{}'s plugin registry at run time. Because this is the way
users interact with this code, it is also the framework within which we have
to approach testing: tests need to consist of specially crafted input files
and/or plugins.


\section{Make it simple for me: A model problem}

We're not going to try demonstrating our approach using \aspect{} -- that
would be far to complex to install and deal with. Rather, we're going to show
how it works with a much simpler code that just deals with a model problem but
that we will write with the same approach towards input handling and testing
as \aspect{}.

So this is what we're going to consider: Imagine you are dealing with two
baseballs (or, if you're living in the ``rest of the world'', golf balls) that
are connected by a spring. If you throw them, what are their trajectories?
Newton's law says that for each of the two balls, mass times acceleration
equals the force on the body. Let's assume for a moment that there is no air
friction and that the spring is massless, then the differential equation that
describes the motion of each of the two bodies ($i=1,2$) is
\begin{align}
  \label{eq:ode}
  m_i 
  \underbrace{\mathbf x_i''(t)}_{\text{acceleration}}
  &=
  \underbrace{m_i \mathbf g}_{\text{gravity}}
  -
  \underbrace{D (\|\mathbf x_2(t) - \mathbf x_1(t)\| - L)}_{\text{magnitude of
    spring force}}
\underbrace{\mathbf d_i}_{\text{direction of spring force}},
\\
\label{eq:ic1}
  \mathbf x_i(0) &= \mathbf x_{i,0},
\\
\label{eq:ic2}
  \mathbf x_i'(0) &= \mathbf v_{i,0},
\end{align}
where $\mathbf x_i(t)$ is the position of the $i$th body at time $t$, $\mathbf
g=(0,0,-g)^T$ is the gravity acceleration, $D$ is the spring constant of
the spring that connects the two balls, and $L$ is the rest length of the
spring. The direction vector $\mathbf d_i$ is
$\mathbf d_1 = \frac{\mathbf x_2(t) - \mathbf x_1(t)}{\|\mathbf x_2(t) - \mathbf
    x_1(t)\|}$
and
$\mathbf d_2 = \frac{\mathbf x_1(t) - \mathbf x_2(t)}{\|\mathbf x_1(t) - \mathbf
    x_2(t)\|}=-\mathbf d_1$ for the two masses.
The second and third equations are the necessary initial conditions
for this second order differential equation and denote the initial position
$\mathbf x_{i,0}$ and initial velocity $\mathbf v_{i,0}$.

\section{A first program}

Let's implement a program that can solve these questions. Because we want it
to be simple and accessible, we're going to use Python as it's widely used and
because it comes with a number of tools that are going to make our life
simpler by keeping the program short. Here it is:
\begin{lstlisting}[frame=single,basicstyle=\footnotesize]
  import json
  ...
\end{lstlisting}
The program reads its input parameters from a file in JSON (see
\url{http://en.wikipedia.org/wiki/JSON}) format.%
\footnote{JSON is not the prettiest format to write input files in and we
  don't recommend you use it for your programs (it is typically intended for
  exchange of data between programs, not between humans and computers). But it
  will serve for our purposes here primarily because Python has a built-in
  parser for it that makes our program pleasantly free of the tedium of
  parsing input files.}
Here is an example file that you can find in the file \texttt{testcase-1.json}:
\lstinputlisting[frame=single,basicstyle=\footnotesize]{tests/testcase-1.json}
In JSON, the name of a parameter is to the left of the colon and its value is
to the right; name/value pairs are separated by commas. The value of a
parameter can be a plain number, or it can be an array if enclosed in
brackets. Arrays can also be nested, which is what we use here for the initial
positions and velocities of our two bodies, each of which lives in
three-dimensional space. With this explanation, it is clear that the input file
specifies properties of the two masses as $\mathbf x_{1,0}=(1,2,3)^T, \mathbf
x_{2,0}=(4,5,6)^T, \mathbf v_{1,0}=(0,0,0)^T, \mathbf v_{2,0}=(1,1,1)^T,
m_1=13.5, m_2=29.75$ and $D=42, L=2.25$ for the spring that connects them, 
thereby completely determining everything in equations
\eqref{eq:ode}--\eqref{eq:ic2}.

When you run the program on this data, it will compute the trajectory of the
two connected balls over a number of seconds.

...maybe some output...


\section{Testing for perpetuity}

So how does one test a code like this? There are really two answers to
this. The first has to do with ensuring that the functionality we have just
implemented is correct. The second is how we can ensure that the functionality
we have implemented today will still work correctly tomorrow after
implementing developing more features.

We often have an intuitive idea of how to do the former. For example, we could
compare with another code, or we could look at special cases for which we have
an analytic solution. For our model problem, one could for example prescribe
initial velocities so that the bodies are at rest and initial positions so
that the two bodies are separate by the rest length of the spring; the two
bodies then simply fall straight down and we can easily compute on a piece of
paper where they should be at time $t$. The following input file
(\texttt{testcase-2.json}) does this:
\lstinputlisting[frame=single,basicstyle=\footnotesize]{tests/testcase-2.json}

In the end, verifying whether the program is correct is problem dependent. Let
us not go too deep into this because the second answer -- how to ensure that
functionality continues to work -- is actually much more important in practice
even though few people realize this when they start developing software. The
issue essentially boils down to this: I need mechanisms that ensure that
functionality if I have verified today works correctly, does not break when
implementing new functionality. This is necessary because good programmers
understand that they will make mistakes; what makes them good programmers is
that they develop ways to minimize the impact by helping them find problems
quickly. 

There is a lot of research on bugs in programs. Fundamentally, the essence of
this research is that it cannot be avoided altogether when programming. On the
other hand, the best programs are those that make it easy to spot bugs and fix
them quickly. The worst programs are those that are not tested; an empirically
true statement is that programs that are not tested produce wrong answers --
where the emphasis is on the fact that they \textit{produce} wrong answers,
not that they \textit{may produce} wrong answers. Experienced programmers all
attest to this statement. Only bad or inexperienced programmers will assume
that they do not introduce new bugs in old parts of their programs.%
\footnote{In fact, what seems to set good programmers apart more than many
  other factor is not that they create fewer bugs -- which they do, though not
  dramatically so -- but that they understand that they create bugs and that
  they change their behavior to accommodate for it, for example by developing
  a habit of rigorous testing that helps them find these bugs quickly.}

The importance of testing relies in the cost of fixing problems. If we keep
implementing new functionality, eventually some old feature will stop
working. If we have no tests that detect this quickly, this may go unnoticed
for a while but eventually we will try to re-run an old computation with the
new version of the code and it will not work any more. So where to start? What
have we changed in the intervening months that may cause this? There is likely
a lot of new code we have written since then, and we may have forgotten about
a fair share of the details since then. It will take a long time to figure out
where exactly the problem is. On the other hand, if we had a testsuite that we
run after essentially every change, we will be notified that something stopped
working immediately -- with a relatively small number of changes made since
the test last worked, and everything still fresh on our mind, finding the bug
should be a much quicker issue.

So how do we achieve this? The general idea is that we save the test cases we
have previously used to convince ourselves that a new part of the program was
correct. To make this work, we have to have three things:
\begin{enumerate}
\item The input file for a previously verified testcase.
\item The output when running the code on this input file; having verified the
  feature, we know or at least believe that this output is correct.
\item A way to run a new version of the code on the output file, to compare
  its current output against the stored output, and to raise a flag if they
  are not the same.
\end{enumerate}
The first two are really not very complicated -- create a directory
\texttt{tests/} in your project and create two files in it, say
\texttt{testcase-1.json} and \texttt{testcase-1.output}. The last part can
be as easy as running commands such as
\begin{lstlisting}[frame=single,basicstyle=\footnotesize,language=csh]
  python masses-and-springs.py testcase-1.json > testcase-1.output.new
  diff testcase-1.output testcase-1.output.new
\end{lstlisting}
You would repeat all of this for \texttt{testcase-2.json}, of course.
If the \texttt{diff} program produces no output, then the two files are the
same and your program computed the same answer on this input file as it did
back when you created the \texttt{testcase-1.output} file. If there is a
difference, then you know that you just broke one of the testcases. If you
purposefully changed it -- for example because you changed the output format,
or because you fixed behavior you know was erroneous and have verified that it
is now correct, then copy the new output over the new one. If you did not
purposefully change it, then you know it's time to break out the debugger.


\section{More functionality}

We write tests because we expect that our code base will continue to change --
to implement new functionality, or to fix existing bugs. So let's do this:
assume we now also want to include air friction on the two balls, and we will
use that the air friction force is proportional to the square of the speed and
in the opposite direction of the velocity \cite{...}. Then our model
\eqref{eq:ode} needs to change to the following:
\begin{align}
  \label{eq:ode2}
  m_i 
  \mathbf x_i''(t)
  &=
  m_i \mathbf g
  -
  D \left(\|\mathbf x_2(t) - \mathbf x_1(t)\| - L\right) \mathbf d_i
  \underbrace{-C_i \; \|\mathbf x_i(t)\| \; \mathbf x_i(t)}_{\text{friction force}}.
\end{align}
The new term contains constants $C_i$ that denote air friction coefficients of
the two balls. These coefficients can be computed from the cross section of
the ball, their surface roughness, the viscosity of the air, and other
factors, but this is not important here -- we will simply read them from the
input file.

If we now require that every input file contains an array of friction
coefficients $C_i$, we can no longer use old input files. A better strategy is
to define default values for all non-required parameters and overwrite them
with the values in the input file if specified. A backward compatible choice
of input parameter for the $C_i$ is to give them a default value of zero
because in that case equation \eqref{eq:ode2} equals the previous version,
\eqref{eq:ode}.

Implementing all of this leads to the following program:
\begin{lstlisting}[frame=single,basicstyle=\footnotesize]
  ...
\end{lstlisting}

How do we test this? First, we should check whether the old input files still
work:
\begin{lstlisting}[frame=single,basicstyle=\footnotesize,language=csh]
  python masses-and-springs.py testcase-1.json > testcase-1.output.new
  diff testcase-1.output testcase-1.output.new
  python masses-and-springs.py testcase-2.json > testcase-2.output.new
  diff testcase-2.output testcase-2.output.new
\end{lstlisting}
With the code that accompanies this paper, \texttt{diff} reports no difference
between the old and the new output files, so this is reassuring. Apparently
the trick with 

Next, we need to write new tests for the new functionality. This is where the
\textit{clone and graft} technique finally comes into play: take one of the
existing tests (i.e., \textit{clone} it) and add to it the minimal number of
parameters to exercise the new functionality (i.e., \textit{graft} onto
it). So let's take the simpler of the two previous testcases,
\texttt{tests/testcase-2.json}, copy it to \texttt{tests/testcase-3.json} and
add to it something about air friction: 
\lstinputlisting[frame=single,basicstyle=\footnotesize]{tests/testcase-3.json}
To verify that the results we get are correct, we can use that in this test
case the masses and friction coefficients of the two balls are the same, the
two balls are separated by a distance equal to the rest length of the spring,
and they start at rest. Then they will simply fall as if there was no spring,
and we know that they will approach a terminal speed $v=\sqrt{mg/C}$ where
downward gravity and upward air friction cancel. For the current case, this
would be $v=\sqrt{9.81}$ because we have chosen $m=1, g=9.81, C=1$.

Is one new testcase enough? The answer is rarely yes. For example, it is easy
to make indexing errors and choosing masses and friction coefficients equal
would not allow us to detect errors where, for example, we used
\texttt{friction[1]} in the code where we intended to use \texttt{friction[i]}.
So let's copy \texttt{tests/testcase-3.json} to
\texttt{tests/testcase-4.json}:
\lstinputlisting[frame=single,basicstyle=\footnotesize]{tests/testcase-4.json}
Here, the second ball has a greater mass, but the same air friction
coefficient (think of it as denser but with the same diameter). How do we know
whether our code is correct for this case as well? This may be more difficult
to establish, but it isn't always necessary. For example, we could look at the
output and realize that the second ball falls faster and, because it is
connected to the first by a spring, then starts to drag the first behind it as
it falls. Such qualitative verification is sometimes the best one can do. At
the same time, while it may not guarantee that the code is indeed correct, it
will nonetheless serve as a useful test in the future to ensure that existing
functionality does not break as a result of further development -- in the long
term the more important of the two reasons to write tests.

Because it is so easy to create tests, it may be worthwhile cloning
\texttt{tests/testcase-3.json} a second time into
\texttt{tests/testcase-5.json} where now we just set the friction coefficients
differently:
\lstinputlisting[frame=single,basicstyle=\footnotesize]{tests/testcase-4.json}
Now the two balls have the same mass but the second one has a higher drag
coefficient (think of the first ball as smooth and the second one as covered
in fur). Our ``smell check'' would now be to verify that the second ball drags
behind the first.

\section{Practical considerations}

\subsection{When clone and graft fails}

\begin{wrapfigure}{R}{0.2\textwidth}
  \begin{center}
    \vspace*{-24pt}
    \includegraphics[width=0.1\textwidth]{figures/inheritance.png}
    \vspace*{-12pt}
  \end{center}
  \caption{\it Inheritance graph for the 5 tests discussed in previous sections.}
  \vspace*{-3mm}
  \label{fig:inheritance}
\end{wrapfigure}
The scheme outlined above represents an inheritance relationship where every
test is based on a previous one. For the 5 testcases discussed above, the
inheritance graph is shown in Fig.~\ref{fig:inheritance}. It is, of course, a
\textit{tree graph}.

But this does not always work. There are cases where you will have to write a
new test from scratch. To give an example of where this was necessary,
consider \aspect{}: imagine that at the beginning, it could only compute
convection in a box. We would have testcases that described different
temperature boundary conditions on this box, and various other things. But
when implementing a spherical geometry, none of these testcases quite fit:
they all had to say what the temperature is on the six faces of the box, they
made the implicit assumption that gravity points downward instead of inward,
etc. It was time to start with a new testcase. If, at a later time you then
clone and graft from this new testcase, then it becomes the root of a separate
tree (let's call it an \textit{exemplar} testcase) and the inheritance graph
will turn into a \textit{forest graph} (a collection of tree graphs).

\begin{wrapfigure}{L}{0.45\textwidth}
  \begin{center}
    \vspace*{-24pt}
    \includegraphics[height=0.45\textwidth,angle=-90]{figures/graph.png}
    \vspace*{-12pt}
  \end{center}
  \caption{\it Connection graph of the 144 tests that are part of the
    \aspect{} code base.}
  \vspace*{-3mm}
  \label{fig:aspect-tests}
\end{wrapfigure}
Finally, there are cases where you copy-paste pieces from multiple existing
tests into a new one, for example because you want to test the combination of
features for which you have already written individual testcases. At this
point, the metaphor of inheritance graphs starts to break down. However, in
practice, most tests are still closely related to some others. A visualization
of this for the 144 tests that are part of \aspect{}, each represented by a
dot, is shown in Fig.~\ref{fig:aspect-tests} where we draw tests farther apart
of there are more textual differences between the input files. Many tests come
in groups of 2, 3 or 4 that only differ in a few lines of input -- simple
modifications such as those between testcases 3, 4 and 5 above. On the other
hand, other group are further apart and are derived from different
examplars.

The message from this section, though, is that in almost all cases writing a
new testcase is actually really easy: find a good starting point, copy it,
make minor modifications and additions, and verify the output. Testing
scientific codes really doesn't have to be hard!

\subsection{Dealing with many tests}

In practice, once you have accumulated a number of testcases, you will
encounter practical difficulties. First, when you try to find the right
starting point to clone and graft a new testcase, you will not recall if
\texttt{tests/testcase-3.json} or \texttt{tests/testcase-5.json} is the one
that's closest to the situation you have in mind (for example, if you want to
augment your model and you are looking again for a testcase with equal masses
and friction coefficients). 

In our experience, using more descriptive filenames for the input files has
helped, as is placing comments at the top of each input file describing what
the test does or, if appropriate, how it differs from a previous
testcase. Many of our testcases in \aspect{} have names like
\texttt{tests/melt-fraction-peridotite.prm} and comments at the top of the
form ``\textit{Like the melt-fraction.prm test, but using the peridotite
  material parameters.}''

Second, running and
comparing many tests by hand using the commands above becomes unwieldy. In such
cases, it is useful to write little \texttt{Makefile}s that do this
automatically, or to use one of the tools available as open source -- for
example, the \texttt{CTest} program. It also
becomes more difficult to display tests that create difference when, for
example, you run the 7,000 tests of the \dealii{} testsuite. Again, there are
freely available systems such as \texttt{CDash} that present graphical
overviews of all succeeding and failing tests in a browser.


\subsection{Dealing with round-off}

Talk about numdiff


\section{Conclusions}



\end{document}
